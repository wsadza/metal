#!/usr/bin/env ansible-playbook
############################################################
# Copyright (c) 2024 wsadza
# Released under the MIT license
# ----------------------------------------------------------
#
#  Ansible playbok to automate installation of:
#  - nvidia-driver
#  - nvidia-device-plugin
#  - docker
#  - nvidia-container-toolkit
#  - k3s
#
############################################################
---

- hosts: target 
 
  become:       false 
  gather_facts: false 

  roles:
  
  #######################################
  # Miscellaneous 
  #######################################
  
  - role: miscs
    package:
      - curl
      - libc6:i386
      - libc6
      - pkg-config 
      - libglvnd-dev 
      - dkms 
      - build-essential 
      - libegl-dev 
      - libegl1 
      - libgl-dev 
      - libgl1 
      - libgles-dev 
      - libgles1 
      - libglvnd-core-dev 
      - libglx-dev 
      - libopengl-dev 
      - gcc 
      - make

  #######################################
  # Nvidia-Driver
  #######################################
  
  - role: script_based
# INSTALLATION 
    installation:
      name: nvidia-driver
      check_command: "nvidia-smi"
      steps:
# Fetching latest Nvidia Drivers
      - |
        NVIDIA_LATEST="$(
          curl -s -X GET https://download.nvidia.com/XFree86/Linux-x86_64/latest.txt | sed 's/^.* //'
        )"; 
        curl -X GET -o /tmp/nvidia.run \
          https://download.nvidia.com/XFree86/Linux-x86_64/${NVIDIA_LATEST}

# Installing Nvidia Drivers
      - |
        /tmp/nvidia.run \
          --silent \
          --install-compat32-libs \
          --no-kernel-module

# Uninstalling i386 architecture
      - |
        dpkg --remove-architecture i386

# CONFIGURATION 
    configuration: []

  #######################################
  # Docker 
  #######################################
  
  - role: aptitude_based

# INSTALLATION 
    installation:
      name: docker 
      check_command: "docker --version"
      gpg_key_url: "https://download.docker.com/linux/$(. /etc/os-release && echo $ID)/gpg"
      repo_url: "https://download.docker.com/linux/$(. /etc/os-release && echo $ID) $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable"
      packages:
      - docker-ce 
      - docker-ce-cli 
      - containerd.io 
      - docker-buildx-plugin 
      - docker-compose-plugin

# CONFIGURATION 
    configuration:
     - |
       mkdir -p ~/.local/share/bash-completion/completions
       docker completion bash > ~/.local/share/bash-completion/completions/docker

  #######################################
  # Helm 
  #######################################

  - role: aptitude_based 

# INSTALLATION 
    installation:
      name: helm 
      check_command: "helm version"
      gpg_key_url: "https://baltocdn.com/helm/signing.asc"
      repo_url: "https://baltocdn.com/helm/stable/debian/ all main"
      packages: 
      - helm 

# CONFIGURATION 
    configuration:
# Configure helm completion 
    - |
      helm completion bash | tee /etc/bash_completion.d/helm > /dev/null
  
  #######################################
  # Kubernetes Client
  #######################################

  - role: aptitude_based

# INSTALLATION 
    installation:
      name: kubectl 
      check_command: "kubectl version"
      gpg_key_url: "https://pkgs.k8s.io/core:/stable:/$(curl -L -s https://dl.k8s.io/release/stable.txt | sed 's/\\.[^.]*$//')/deb/Release.key"
      repo_url: "https://pkgs.k8s.io/core:/stable:/$(curl -L -s https://dl.k8s.io/release/stable.txt | sed 's/\\.[^.]*$//')/deb/ /"
      packages: 
      - kubectl 

# CONFIGURATION 
    configuration:
# Configure bash completion 
    - |
      kubectl completion bash | tee /etc/bash_completion.d/kubectl > /dev/null
  
  #######################################
  # K9S 
  #######################################

  - role: script_based
# INSTALLATION 
    installation:
      name: k9s
      check_command: "k9s info"
      steps:
      - |
        curl -sS https://webinstall.dev/k9s | bash

# CONFIGURATION 
    configuration: []

  #######################################
  # K3S
  #######################################

  - role: script_based
# INSTALLATION 
    installation:
      name: k3s
      check_command: "k3s --version"
      steps:
# Install K3S 
      - |
        curl -sfL https://get.k3s.io | sh -s -
# Expose Kubeconfig for user 
      - |
        mkdir -p ~/.kube
        cp /etc/rancher/k3s/k3s.yaml ~/.kube/config

# CONFIGURATION 
    configuration: []

  #######################################
  # Nvidia Container Toolkit
  #######################################

  - role: aptitude_based

# INSTALLATION 
    installation:
      name: nvidia-container-toolkit
      check_command: "nvidia-container-toolkit --version"
      gpg_key_url: "https://nvidia.github.io/libnvidia-container/gpgkey"
      repo_url: 'https://nvidia.github.io/libnvidia-container/stable/deb/\$(ARCH) /'
      packages: 
      - nvidia-container-toolkit

# CONFIGURATION 
    configuration:
# Configure Nvidia Runtime for Containerd 
    - |
      nvidia-ctk runtime configure --runtime=containerd

# Configure Nvidia Runtime for Docker 
    - |
      nvidia-ctk runtime configure --runtime=docker

  #######################################
  # Nvidia Device Plugin 
  #######################################

  - role: script_based
# INSTALLATION 
    installation:
      name: nvidia-device-plugin 
      check_command: ""
      steps:
# Create Nvidia Runtime Class 
      - |
        cat <<EOF | kubectl apply -f -
        ---
        apiVersion: node.k8s.io/v1
        kind: RuntimeClass
        metadata:
          name: nvidia
        handler: nvidia
        EOF
# Add HELM repo
      - |
        helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
# Install Nvidia Device Plugin 
      - |
        helm upgrade -i nvidia-device-plugin nvdp/nvidia-device-plugin \
          --version 0.16.0 \
          --set gfd.enabled=true \
          --set runtimeClassName=nvidia \
          --set config.name=gpu-sharing-config \
          --namespace nvidia-device-plugin \
          --create-namespace 
# Create Nvidia ConfigMap (share GPU)
      - |
        cat <<EOF | kubectl apply -f -
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: gpu-sharing-config
          namespace: nvidia-device-plugin
        data:
          config.yaml: |
            version: v1
            sharing:
              mps:
                resources:
                  - name: nvidia.com/gpu
                    replicas: 5 
        EOF
# Label Node 
      - |
        NODE="$(kubectl get nodes --output=jsonpath='{.items[*].metadata.name}')"
        kubectl label node ${NODE} nvidia.com/mps.capable=true	

# CONFIGURATION 
    configuration: []
...
